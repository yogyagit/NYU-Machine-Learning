# -*- coding: utf-8 -*-
"""ys5250 ML Assignment 2 Q1 Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQNmXbJze6GviY56ISIAYqBhyOerDFF0
"""

import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#importing dataset
dataset = scipy.io.loadmat('data3.mat')

#creating a dataframe and adding the bias parameter x0 = 1 to it
df = pd.DataFrame(dataset['data'] , columns = ['x1' , 'x2' , 'y'])
df['x0'] = 1.0
df = df.iloc[ : , [3, 0, 1, 2]]

#Extractiong X and Y from the dataframe
X = pd.DataFrame(df.iloc[ : , :3])
Y = pd.DataFrame(df.iloc[ : , 3:4])

#initializing model parameter values using randomize operation
T = pd.DataFrame(np.random.randint(200 , size = (3)))

#declaring a list to store binary classifictaion error for each iteration
binaryClassificationError = list()

#declaring a list to store perceptron loss for each iteration
perceptronLoss = list()

misclassCheck = list()

#Setting up iteration counter
iterations = 0
check = 1
falseCheck = 1

#Starting to loop to calculate model parameters and corresponding error and risk
while check:
  
  #Checking if there is any misclassifiction after the previous value of model parameters
  #starts from the second iteration
  if iterations > 0:
    falseCheck = 0
    for g in range(len(misclassCheck[0])):
      if misclassCheck[0][g] == 'false':
        falseCheck = falseCheck+1

  if falseCheck != 0:

    count = 0
    iterations = iterations+1
    misclassCheck.clear()
    
    #calculating the predictions
    xT = pd.DataFrame(np.dot( X , T ) , columns = ['xT'])
    
    predictions = list()
    
    #appending the newly calculated labels in a list
    predictions.append(np.where(xT < 0, -1.0 , 1.0))

    pred = pd.DataFrame(predictions[0], columns = ['predictions'])

    #appending misclassCheck list by if the predicted value is true or false
    misclassCheck.append(np.where(pred['predictions'] == Y['y'], 'true', 'false'))
 
    #calculating binary classifiction error
    for i in range(len(misclassCheck[0])): 
      if misclassCheck[0][i] == 'false':
        count = count + 1
    
    error = count/len(X)
  
    binaryClassificationError.append(error)
    
    falseIndex = []

    #Checking which indexes have been misclassified
    falseIndex = np.where(misclassCheck[0] == 'false')[0]
    
    risk = 0
    err = 0
    
    #calculating perceptron loss
    for j in falseIndex:
      err = err + np.dot(xT.loc[[j]], Y.loc[[j]])[0][0]
   
    risk = -(err)/len(X)

    perceptronLoss.append(risk)
    
    #evaluating model parameters
    if count != 0:
      T = T + np.dot(X.iloc[[falseIndex[0]]].T , Y.iloc[[falseIndex[0]]] ) 
      check = 1
    else:
      check = 0
      print("Final model parameter values", T)

print("Number of iterations:", iterations)
#Plotting scatter plot for y values
figure, hlt = plt.subplots(1,3)
for r in [-1, 1]:
    row = np.where(df.y == r)
    hlt[0].scatter( df.iloc[row[0],1], df.iloc[row[0], 2], s= 0.5 )

hlt[0].set_title("Scatter Plot for Training Data")
hlt[0].set_xlabel("x1")
hlt[0].set_ylabel("x2")

#plotting decision boundary
decisionBoundary = - (float(T.iloc[1])/float(T.iloc[2])) * X.iloc[:,1] - (float(T.iloc[0])/float(T.iloc[2]))
hlt[0].plot(X.iloc[:,1], decisionBoundary, 'y')

#plotting risk vs. iterations
hlt[1].plot(range(iterations), perceptronLoss, '.', color = 'red')
hlt[1].set_title("Risk vs. Number of Iterations")
hlt[1].set_xlabel("Number of Iterations")
hlt[1].set_ylabel("Risk")

#plotting binary classifictaion error vs. iterations
hlt[2].plot(range(iterations), binaryClassificationError, '.', color = 'blue')
hlt[2].set_title("Binary Classification Error vs. Number of Iterations")
hlt[2].set_xlabel("Number of Iterations")
hlt[2].set_ylabel("Binary Classification Error")

plt.subplots_adjust(left=0.3,
                    bottom=0.2,
                    right=3.0,
                    top=0.9,
                    wspace=0.4,
                    hspace=0.4)


plt.grid()
plt.show()